{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import rospy\n",
    "import time\n",
    "import actionlib\n",
    "from control_msgs.msg import FollowJointTrajectoryAction, FollowJointTrajectoryActionGoal, FollowJointTrajectoryGoal\n",
    "from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\n",
    "from sensor_msgs.msg import JointState\n",
    "from std_srvs.srv import Empty\n",
    "from gazebo_msgs.srv import GetLinkState, GetLinkStateRequest\n",
    "from gazebo_msgs.srv import GetModelState, GetModelStateRequest\n",
    "\n",
    "import numpy as np\n",
    "a = 0\n",
    "def joint_state_subscriber_callback(js):\n",
    "    print(js.position)\n",
    "    \n",
    "rospy.init_node('nodenode')\n",
    "get_model_state_proxy = rospy.ServiceProxy('/gazebo/get_model_state',GetModelState)\n",
    "get_model_state_req = GetModelStateRequest()\n",
    "get_model_state_req.model_name = 'quadruped'\n",
    "get_model_state_req.relative_entity_name = 'world'\n",
    "rospy.wait_for_service('/gazebo/get_model_state')\n",
    "model_state = get_model_state_proxy(get_model_state_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "header: \n",
       "  seq: 1\n",
       "  stamp: \n",
       "    secs: 805\n",
       "    nsecs: 819000000\n",
       "  frame_id: \"world\"\n",
       "pose: \n",
       "  position: \n",
       "    x: -0.16866895249\n",
       "    y: -0.194112939989\n",
       "    z: 0.177245674712\n",
       "  orientation: \n",
       "    x: 3.19472021982e-05\n",
       "    y: -2.01681959123e-06\n",
       "    z: -0.0395837178888\n",
       "    w: 0.999216257\n",
       "twist: \n",
       "  linear: \n",
       "    x: -0.000476032743121\n",
       "    y: -0.000302369679464\n",
       "    z: -0.000630571065311\n",
       "  angular: \n",
       "    x: 1.33394654721e-05\n",
       "    y: -0.00073679690491\n",
       "    z: 0.000141591648308\n",
       "success: True\n",
       "status_message: \"GetModelState: got properties\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import rospy\n",
    "import time\n",
    "import actionlib\n",
    "from control_msgs.msg import FollowJointTrajectoryAction, FollowJointTrajectoryActionGoal, FollowJointTrajectoryGoal\n",
    "from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\n",
    "from std_srvs.srv import Empty\n",
    "from gazebo_msgs.srv import GetLinkState, GetLinkStateRequest\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "class AllJoints:\n",
    "    def __init__(self):\n",
    "        self.jta = actionlib.SimpleActionClient('/quadruped/joint_trajectory_controller/follow_joint_trajectory',\n",
    "                                                FollowJointTrajectoryAction)\n",
    "        rospy.loginfo('Waiting for joint trajectory action')\n",
    "        self.jta.wait_for_server()\n",
    "        rospy.loginfo('Found joint trajectory action!')\n",
    "        self.msg = FollowJointTrajectoryActionGoal()\n",
    "\n",
    "    def move(self, pos):\n",
    "        msg = FollowJointTrajectoryActionGoal()\n",
    "        msg.goal.trajectory.joint_names = ['front_right_leg1_joint', 'front_right_leg2_joint', 'front_right_leg3_joint',\n",
    "                                           'front_left_leg1_joint', 'front_left_leg2_joint', 'front_left_leg3_joint',\n",
    "                                           'back_right_leg1_joint', 'back_right_leg2_joint','back_right_leg3_joint',\n",
    "                                           'back_left_leg1_joint', 'back_left_leg2_joint', 'back_left_leg3_joint']\n",
    "        point = JointTrajectoryPoint()\n",
    "        point.positions = pos\n",
    "        point.time_from_start = rospy.Duration(1.0/60.0)\n",
    "        msg.goal.trajectory.points.append(point)\n",
    "        self.jta.send_goal_and_wait(msg.goal)\n",
    "    \n",
    "class QuadrupedEnvironment:\n",
    "    def __init__(self):\n",
    "        rospy.init_node('joint_position_node')\n",
    "        self.nb_joints = 12\n",
    "        self.nb_links = 13\n",
    "        self.link_name_lst = ['quadruped::base_link',\n",
    "                             'quadruped::front_right_leg1', 'quadruped::front_right_leg2', 'quadruped::front_right_leg3',\n",
    "                             'quadruped::front_left_leg1', 'quadruped::front_left_leg2', 'quadruped::front_left_leg3',\n",
    "                             'quadruped::back_right_leg1', 'quadruped::back_right_leg2', 'quadruped::back_right_leg3',\n",
    "                             'quadruped::back_left_leg1', 'quadruped::back_left_leg2', 'quadruped::back_left_leg3']\n",
    "        self.leg_link_name_lst = self.link_name_lst[1:]\n",
    "        self.all_joints = AllJoints()\n",
    "        self.starting_pos = np.array([0.8, 0.6, 0.0,\n",
    "                                     0.8, 0.6, 0.0,\n",
    "                                     0.8, 0.6, 0.0,\n",
    "                                     0.8, 0.6, 0.0])\n",
    "        \n",
    "        self.pause_proxy = rospy.ServiceProxy('/gazebo/pause_physics',Empty)\n",
    "        self.unpause_proxy = rospy.ServiceProxy('/gazebo/unpause_physics',Empty)\n",
    "        self.reset_proxy = rospy.ServiceProxy('/gazebo/reset_simulation',Empty)\n",
    "        self.get_state_proxy = rospy.ServiceProxy('/gazebo/get_link_state',GetLinkState)\n",
    "        self.get_state_req = GetLinkStateRequest()\n",
    "        \n",
    "        self.joint_pos_high = np.array([1.0, 1.0, 1.0,\n",
    "                                     1.0, 1.0, 1.0,\n",
    "                                     1.0, 1.0, 1.0,\n",
    "                                     1.0, 1.0, 1.0])\n",
    "        self.joint_pos_low = np.array([-0.5, 0.0, -1.0,\n",
    "                                    -0.5, 0.0, -1.0,\n",
    "                                    -0.5, 0.0, -1.0,\n",
    "                                    -0.5, 0.0, -1.0])\n",
    "        self.joint_pos_range = self.joint_pos_high - self.joint_pos_low\n",
    "        self.joint_pos_mid = (self.joint_pos_high + self.joint_pos_low) / 2.0\n",
    "        self.joint_pos = self.starting_pos\n",
    "        self.joint_state = np.zeros(self.nb_joints)\n",
    "        self.state2d = np.zeros((self.nb_joints,6))\n",
    "        self.state = np.zeros(self.nb_joints*6)\n",
    "        self.state_pos_coeff = 100.0\n",
    "        self.state_posdiff_coeff = 10.0\n",
    "        self.base_pos = np.zeros(3)\n",
    "        #self.action_coeff = 0.2\n",
    "        self.reward_coeff = 300.0\n",
    "        self.reward = 0.0\n",
    "        self.done = False\n",
    "        self.max_sim_time = 15.0\n",
    "        \n",
    "    def normalize_joint_pos(self,joint_pos):\n",
    "        return (joint_pos - self.joint_pos_mid)/(self.joint_pos_range/2.0)\n",
    "    def joint_state_subscriber_callback(self,joint_state):\n",
    "        self.joint_state = \n",
    "        \n",
    "    def reset(self):\n",
    "        rospy.wait_for_service('/gazebo/pause_physics')\n",
    "        try:\n",
    "            self.pause_proxy()\n",
    "        except rospy.ServiceException, e:\n",
    "            print('/gazebo/pause_physics service call failed')\n",
    "        rospy.wait_for_service('/gazebo/reset_simulation')\n",
    "        try:\n",
    "            self.reset_proxy()\n",
    "        except rospy.ServiceException, e:\n",
    "            print('/gazebo/reset_simulation service call failed')\n",
    "        rospy.wait_for_service('/gazebo/unpause_physics')\n",
    "        try:\n",
    "            self.unpause_proxy()\n",
    "        except rospy.ServiceException, e:\n",
    "            print('/gazebo/unpause_physics service call failed')\n",
    "        self.all_joints.move(self.starting_pos)\n",
    "        self.joint_pos = self.starting_pos\n",
    "        self.state2d = np.zeros((self.nb_joints,2))\n",
    "        \n",
    "        \n",
    "        self.base_pos = base_link_pos\n",
    "        self.reward = 0.0\n",
    "        self.state = self.state2d.reshape((1,-1))\n",
    "        done = False\n",
    "        time.sleep(1)\n",
    "        return self.state, done\n",
    "        \n",
    "    def step(self, action):\n",
    "        action = action * self.joint_pos_range\n",
    "        print('action:',action)\n",
    "        self.joint_pos = np.clip(self.joint_pos + action,a_min=self.joint_pos_low,a_max=self.joint_pos_high)\n",
    "        self.all_joints.move(self.joint_pos)\n",
    "        print('joint pos:',self.joint_pos)\n",
    "        rospy.wait_for_service('/gazebo/get_link_state')\n",
    "        self.get_state_req.link_name = self.link_name_lst[0]\n",
    "        model_state = self.get_state_proxy(self.get_state_req)\n",
    "        base_link_pos = np.array([model_state.link_state.pose.position.x,\n",
    "                                 model_state.link_state.pose.position.y,\n",
    "                                 model_state.link_state.pose.position.z])\n",
    "        \n",
    "        for i, link_name in enumerate(self.leg_link_name_lst):\n",
    "            self.get_state_req.link_name = link_name\n",
    "            model_state = self.get_state_proxy(self.get_state_req)\n",
    "            pos = (np.array([model_state.link_state.pose.position.x,\n",
    "                                 model_state.link_state.pose.position.y,\n",
    "                                 model_state.link_state.pose.position.z])\n",
    "                  - base_link_pos)\n",
    "            self.state2d[i,3:] = self.state_posdiff_coeff * (self.state2d[i,:3] - self.state_pos_coeff * pos)\n",
    "            self.state2d[i,:3] = self.state_pos_coeff * pos\n",
    "        self.reward = self.reward_coeff * (base_link_pos[1]-self.base_pos[1] \n",
    "                                           - np.sqrt((base_link_pos[0]-self.base_pos[0])**2\n",
    "                                                     + (base_link_pos[0]-self.base_pos[0])**2))\n",
    "        self.reward += -0.5\n",
    "        self.base_pos = base_link_pos\n",
    "        self.state = self.state2d.reshape((1,-1))\n",
    "        curr_time = rospy.get_time()\n",
    "        print('time:',curr_time)\n",
    "        if curr_time > self.max_sim_time:\n",
    "            done = True\n",
    "            self.reset()\n",
    "        else:\n",
    "            done = False\n",
    "        print('state',self.state)\n",
    "        return self.state, self.reward, done\n",
    "        \n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, maxlen, action_shape, state_shape, dtype=np.float32):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\"\"\"\n",
    "        self.maxlen = maxlen\n",
    "        self.start = 0\n",
    "        self.length = 0\n",
    "        self.state_data = np.zeros((maxlen,) + state_shape).astype(dtype)\n",
    "        self.action_data = np.zeros((maxlen,) + action_shape).astype(dtype)\n",
    "        self.reward_data = np.zeros((maxlen,1)).astype(dtype)\n",
    "        self.next_state_data = np.zeros((maxlen,) + state_shape).astype(dtype)\n",
    "        self.done_data = np.zeros((maxlen,1)).astype(dtype)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        if self.length == self.maxlen:\n",
    "            self.start = (self.start + 1) % self.maxlen\n",
    "        else:\n",
    "            self.length += 1\n",
    "        idx = (self.start + self.length - 1) % self.maxlen\n",
    "        self.state_data[idx] = state\n",
    "        self.action_data[idx] = action\n",
    "        self.reward_data[idx] = reward\n",
    "        self.next_state_data[idx] = next_state\n",
    "        self.done_data[idx] = done\n",
    "    \n",
    "    def sample(self, batch_size=64):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        idxs = np.random.randint(0,self.length - 1, size=batch_size)\n",
    "        sampled = {'states':self.set_min_ndim(self.state_data[idxs]),\n",
    "                   'actions':self.set_min_ndim(self.action_data[idxs]),\n",
    "                   'rewards':self.set_min_ndim(self.reward_data[idxs]),\n",
    "                   'next_states':self.set_min_ndim(self.next_state_data[idxs]),\n",
    "                   'dones':self.set_min_ndim(self.done_data[idxs])}\n",
    "        return sampled\n",
    "    \n",
    "    def set_min_ndim(self,x):\n",
    "        \"\"\"set numpy array minimum dim to 2 (for sampling)\"\"\"\n",
    "        if x.ndim < 2:\n",
    "            return x.reshape(-1,1)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "    #0.15 0.3\n",
    "    def __init__(self, size, mu=None, theta=0.15, sigma=0.03, dt=1e-2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.size = size\n",
    "        self.mu = mu if mu is not None else np.zeros(self.size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.state = np.ones(self.size) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = np.ones(self.size) * self.mu\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "class A2C:\n",
    "    def __init__(self, state_shape, action_shape, actor_lr=0.001, critic_lr=0.001, gamma=0.99):\n",
    "        tf.reset_default_graph()\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.nb_actions = np.prod(self.action_shape)\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        #inputs\n",
    "        self.input_state = tf.placeholder(tf.float32, (None,) + self.state_shape, name='input_state')\n",
    "        self.input_action = tf.placeholder(tf.float32, (None,) + self.action_shape, name='input_action')\n",
    "        self.input_state_target = tf.placeholder(tf.float32, (None,) + self.state_shape, name='input_state_target')\n",
    "        self.rewards = tf.placeholder(tf.float32, (None,1), name='rewards')\n",
    "        self.dones =tf.placeholder(tf.float32, (None,1), name='dones')\n",
    "        \n",
    "        #local and target nets\n",
    "        self.actor = self.actor_net(self.input_state, self.nb_actions,name='actor')\n",
    "        self.critic = self.critic_net(self.input_state, self.input_action,name='critic')\n",
    "        self.actor_and_critic = self.critic_net(self.input_state,self.actor,name='critic',reuse=True)\n",
    "        \n",
    "        self.actor_target = self.actor_net(self.input_state_target, self.nb_actions, name='target_actor')\n",
    "        self.actor_and_critic_target = self.critic_net(self.input_state_target,\n",
    "                                                       self.actor_target, name='target_critic')\n",
    "        \n",
    "        self.actor_loss, self.critic_loss = self.set_model_loss(self.critic, self.actor_and_critic,\n",
    "                                                                self.actor_target, self.actor_and_critic_target,\n",
    "                                                                self.rewards, self.dones, self.gamma)\n",
    "        \n",
    "        self.actor_opt, self.critic_opt = self.set_model_opt(self.actor_loss, self.critic_loss,\n",
    "                                                             self.actor_lr, self.critic_lr)\n",
    "        \n",
    "    \n",
    "    def actor_net(self, state, nb_actions, name, reuse=False, training=True):\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "            x = tf.layers.Dense(16)(state)\n",
    "            x = tf.contrib.layers.layer_norm(x,activation_fn=tf.nn.relu)\n",
    "            x = tf.layers.Dense(16)(x)\n",
    "            x = tf.contrib.layers.layer_norm(x,activation_fn=tf.nn.relu)\n",
    "            actions = tf.layers.Dense(nb_actions,\n",
    "                                      activation=tf.tanh,\n",
    "                                      kernel_initializer=tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3))(x)\n",
    "            return actions\n",
    "    \n",
    "    def critic_net(self, state, action, name, reuse=False, training=True):\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "            x = tf.layers.Dense(16)(state)\n",
    "            x = tf.contrib.layers.layer_norm(x,activation_fn=tf.nn.relu)\n",
    "            x = tf.concat([x, action], axis=-1)\n",
    "            x = tf.layers.Dense(16)(x)\n",
    "            x = tf.contrib.layers.layer_norm(x,activation_fn=tf.nn.relu)\n",
    "            q = tf.layers.Dense(1,kernel_initializer=tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3))(x)\n",
    "            return q\n",
    "    \n",
    "    def set_model_loss(self, critic, actor_and_critic, actor_target, actor_and_critic_target, rewards, dones, gamma):\n",
    "        Q_targets = rewards + (gamma * actor_and_critic_target) * (1. - dones)\n",
    "        actor_loss = tf.reduce_mean(-actor_and_critic)\n",
    "        tf.losses.add_loss(actor_loss)\n",
    "        critic_loss = tf.losses.huber_loss(Q_targets,critic)\n",
    "        return actor_loss, critic_loss\n",
    "    \n",
    "    def set_model_opt(self, actor_loss, critic_loss, actor_lr, critic_lr):\n",
    "        train_vars = tf.trainable_variables()\n",
    "        actor_vars = [var for var in train_vars if var.name.startswith('actor')]\n",
    "        critic_vars = [var for var in train_vars if var.name.startswith('critic')]\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            actor_opt = tf.train.AdamOptimizer(actor_lr).minimize(actor_loss, var_list=actor_vars)\n",
    "            critic_opt = tf.train.AdamOptimizer(critic_lr).minimize(critic_loss, var_list=critic_vars)\n",
    "        return actor_opt, critic_opt\n",
    "    \n",
    "        \n",
    "class DDPG:\n",
    "    \"\"\"Reinforcement Learning agent that learns using DDPG.\"\"\"\n",
    "    def __init__(self,state_shape,action_shape,batch_size=128,gamma=0.995,tau=0.005,actor_lr=0.0001, critic_lr=0.001):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.nb_actions = np.prod(self.action_shape)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.buffer_size = 100000\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ReplayBuffer(self.buffer_size,self.action_shape, self.state_shape)\n",
    "        \n",
    "        # Noise process\n",
    "        self.noise = OUNoise(self.nb_actions)\n",
    "        \n",
    "        # Algorithm parameters\n",
    "        self.gamma = gamma # discount factor\n",
    "        self.tau = tau #soft update\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        \n",
    "        #initialize\n",
    "        self.a2c = A2C(self.state_shape, self.action_shape, actor_lr=self.actor_lr, critic_lr=self.critic_lr,\n",
    "                       gamma=self.gamma)\n",
    "        self.initialize()\n",
    "        \n",
    "        #initial episode vars\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.total_reward = 0.0\n",
    "        self.count = 0\n",
    "        self.episode_num = 0\n",
    "        \n",
    "    def reset_episode_vars(self):\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.total_reward = 0.0\n",
    "        self.count = 0\n",
    "        \n",
    "    def step(self, state, reward, done):\n",
    "        action = self.act(state)\n",
    "        self.count += 1\n",
    "        if self.last_state is not None and self.last_action is not None:\n",
    "            self.total_reward += reward\n",
    "            self.memory.add(self.last_state, self.last_action, reward, state, done)\n",
    "        if (len(self.memory) > self.batch_size):\n",
    "            experiences = self.memory.sample(self.batch_size)\n",
    "            self.learn(experiences)\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        if done:\n",
    "            self.episode_num += 1\n",
    "            print('Episode {}: total reward={:7.4f}, count={}'.format(self.episode_num,self.total_reward,self.count))\n",
    "            self.reset_episode_vars()\n",
    "        return action\n",
    "\n",
    "    def act(self, states):\n",
    "        \"\"\"Returns actions for given state(s) as per current policy.\"\"\"\n",
    "        actions = self.sess.run(self.a2c.actor, feed_dict={self.a2c.input_state:states})\n",
    "        return np.clip(actions + self.noise.sample(),a_min=-1.,a_max=1.).reshape(self.action_shape)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\"\"\"\n",
    "        states = experiences['states']\n",
    "        actions = experiences['actions']\n",
    "        rewards = experiences['rewards']\n",
    "        next_states = experiences['next_states']\n",
    "        dones = experiences['dones']\n",
    "        \n",
    "        #actor critic update\n",
    "        self.sess.run([self.a2c.actor_opt,self.a2c.critic_opt],feed_dict={self.a2c.input_state:states,\n",
    "                                                                              self.a2c.input_action:actions,\n",
    "                                                                              self.a2c.input_state_target:next_states,\n",
    "                                                                              self.a2c.rewards:rewards,\n",
    "                                                                              self.a2c.dones:dones})\n",
    "        #target soft update\n",
    "        self.sess.run(self.soft_update_ops)\n",
    "        \n",
    "        \n",
    "    def initialize(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        actor_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
    "        actor_target_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='target_actor')\n",
    "        critic_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='critic')\n",
    "        critic_target_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='target_critic')\n",
    "        target_init_ops = []\n",
    "        soft_update_ops = []\n",
    "        for var, target_var in zip(actor_var, actor_target_var):\n",
    "            target_init_ops.append(tf.assign(target_var,var))\n",
    "            soft_update_ops.append(tf.assign(target_var, (1. - self.tau) * target_var + self.tau * var))\n",
    "        for var, target_var in zip(critic_var, critic_target_var):\n",
    "            target_init_ops.append(tf.assign(target_var,var))\n",
    "            soft_update_ops.append(tf.assign(target_var, (1. - self.tau) * target_var + self.tau * var))\n",
    "        self.soft_update_ops = soft_update_ops\n",
    "        self.sess.run(target_init_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = QuadrupedEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = env.state.shape\n",
    "action_shape = env.joint_pos.shape\n",
    "agent = DDPG(state_shape,action_shape,batch_size=128,gamma=0.995,tau=0.001,actor_lr=0.0001, critic_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episode = 500\n",
    "observation, done = env.reset()\n",
    "action = agent.act(observation)\n",
    "observation, reward, done = env.step(action)\n",
    "for i in xrange(max_episode):\n",
    "    step_num = 0\n",
    "    while done == False:\n",
    "        step_num += 1\n",
    "        action = agent.step(observation, reward, done)\n",
    "        observation, reward, done = env.step(action)\n",
    "        print('reward:',reward,'step:',step_num, 'episode:', i)\n",
    "    action = agent.step(observation, reward, done)\n",
    "    observation, done = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [- np.ones(12) * ((i%2)*2.0 -1.0) for i in xrange(10)]\n",
    "for p in pos:\n",
    "    env.all_joints.move(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([-0.5, 0.0, -1.0,\n",
    "                                    -0.5, 0.0, -1.0,\n",
    "                                    -0.5, 0.0, -1.0,\n",
    "                                    -0.5, 0.0, -1.0])\n",
    "env.all_joints.move(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
